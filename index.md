---
title: "Personal Website"
name: index
layout: page
description: |
  Personal Website of Ling-Qi, PhD Candidate in Computational Neuroscience at Penn.
hide_description: yes
---

<style type="text/css">
	.page-title {
		position: absolute;
		width: 1px;
  		height: 1px;
  		margin: -1px;
  		border: 0;
  		padding: 0;
  		clip: rect(0 0 0 0);
  		overflow: hidden;
	}
</style>

<h2 class="h1" style="color: rgb(0,0,0)" id="about">About Me </h2>

Hi there! Welcome to my website! My name is Jeesung Ahn, I am a 4th-year Ph.D. candidate in the [Department of Psychology at the University of Pennsylvania](https://psychology.sas.upenn.edu). I am working with [Emily Falk](https://www.asc.upenn.edu/people/faculty/emily-falk-phd) at the [Communication Neuroscience Lab](https://www.asc.upenn.edu/research/centers/communication-neuroscience-lab).

My current research mainly revolves around two topics: 
- using the brain-as-predictor approach to evaluate the effectiveness of persuasive messages in promoting health behaviors (e.g., physical activity)
- understanding how brain networks and social networks are associated with individual differences in healthy lifestyles and mental well-being (e.g., loneliness).

Over the course of 7+ years leading psychology and neuroimaging research projects, I have primarily worked with human behavioral and brain data in the experimental psychology and social neuroscience domains. My research methods are multi-disciplinary and include the followings: 
- experiment and survey design (including A/B testing)
- survey/behavioral/neuroimaging data collection
- descriptive/parametric/non-parametric/multivariate statistics
- general linear modeling
- multilevel modeling
- time-series analysis
- computational network analysis
- machine-learning classification
- meta-analysis of multi-dimensional datasets
- qualitative content analysis.

I am passionate about communicating and translating scientific insights into practical and actionable outcomes in complex decision-making scenarios. To that end, I have been actively involved with various social group activities where I can leverage my research skills to make real-world impacts on matters that I care about (e.g., consulting for a healthcare company with [Penn Biotech Group](https://pennbiotechgroup.org/), analyzing/visualizing data for an [educational diversity and equity initiative](https://web.sas.upenn.edu/dive/), analyzing/visualizing climate-related data to inform climate activists).

---
<h2 class="h1" style="color: rgb(0,0,0)" id="research">Research </h2>
<h3 class="h2">Selected Projects</h3>

**BEHAVIORAL AND NEURAL EFFICIENT CODING OF SPEED**  
*[\[JNeurosci, 2022\]](https://www.jneurosci.org/content/42/14/2951)  [\[V-VSS 2021 Poster\]](https://www.youtube.com/watch?v=W5DH4h2dH8Y)  [\[GitHub\]](https://github.com/lingqiz/Speed_Prior_2021)* <br>

<div class="row">
  <div class="column">
  <img class="proj-image" src="/assets/img/speedPrior.png" style="height: 100%; width: 100%; object-fit: contain">
  </div>

  <div class="column" markdown="1">
  We built an efficient encoding, Bayeisan decoding model for human speed perception in a psychophysical experiment. The model makes specific perdictions regarding the neural encoding characteristics of retinal speed, which we validated by analyzing electrophysiology recording of MT neurons.
  </div>
</div>

<br>
**BAYESIAN IMAGE RECONSTRUCTION FROM CONE MOSAIC SIGNAL**  
*[\[eLife, 2022\]](https://elifesciences.org/articles/71132)  [\[V-VSS 2020 Talk\]](https://youtu.be/d5qI0FNCAv4)  [\[GitHub\]](https://github.com/isetbio/ISETImagePipeline)* <br>

<div class="row">
  <div class="column">
  <img class="proj-image" src="/assets/img/imageRecon.png" style="height: 100%; width: 100%; object-fit: contain">
  </div>

  <div class="column" markdown="1">
  We built a Bayesian image reconstruction algorithm from cone excitations based on an accurate model of human early vision, in order to understand information loss at the very first step of visual processing. Our model enables quantitative analysis of retinal mosaic design, visualization, and the more "traditional" ideal observer type of analysis.
  </div>
</div>

<br>
**VISUAL ORIENTATION ENCODING IN INDIVIDUALS WITH AUTISM**  
*[\[PLOS Biology, 2021\]](https://doi.org/10.1371/journal.pbio.3001215)  [\[Primer\]](https://doi.org/10.1371/journal.pbio.3001293)  [\[Data+Code\]](https://github.com/lingqiz/ASD_Encoding_2020)* <br>

<div class="row">
  <div class="column">
  <img class="proj-image" src="/assets/img/encodingASD.png" style="height: 100%; width: 100%; object-fit: contain">
  </div>

  <div class="column" markdown="1">
  We compared the accuracy of visual orientation encoding between neurotypical and ASD groups using an information theoretic measure. We found that the ASD group starts with an overall lower encoding capacity, which does not improve when presented with performance feedback. They are also less adaptive to the stimulus statistics in contrast to the neurotypical subjects.
  </div>
</div>

<br>
**PSYCHOPHYSICS WITH DEEP NEURAL NETWORKS**  
*[\[CCN, 2019\]](https://ccneuro.org/2019/proceedings/0000585.pdf)* <br>

<div class="row">
  <div class="column">
  <img class="proj-image" src="/assets/img/ccn2019.png" style="height: 100%; width: 100%; object-fit: contain">
  </div>

  <div class="column" markdown="1">
  We showed that pretrained neural networks, like humans, have internal representations that overrepresent frequent variable values at the expense of certainty for less common values. Furthermore, we demonstrated that optimized readouts of local visual orientation from these networks’ internal representations show similar orientation biases and geometric illusions, just as human subjects.
  </div>
</div>

<br>
**COURSE PROJECT (CSE 167x/168x, COMPUTER GRAPHICS)**  
*[\[Simple Python Ray Tracer\]](https://github.com/lingqiz/CSE-167x-RayTracer)  [\[Path Tracer with OptiX\]](https://github.com/lingqiz/CSE-168x-OptiX)* <br>

<div class="row">
  <div class="column">
  <img class="proj-image" src="/assets/img/cse167.png" style="height: 100%; width: 100%; object-fit: contain">
  </div>

  <div class="column" markdown="1">
  Basics of physically based rendering: Implementation of basic Whitted Ray-Tracing with Python, and a Monte Carlo Path Tracer with NVIDIA OptiX.
  </div>
</div>

<br>
**COURSE PROJECT (STAT 927, BAYESIAN STATISTICS)**  
*[\[Data+Code\]](https://github.com/zlqzcc/DoublePassBayesian)* <br>

<div class="row">
  <div class="column">
  <img class="proj-image" src="/assets/img/bayesian.png" style="height: 100%; width: 100%; object-fit: contain">
  </div>

  <div class="column" markdown="1">
  Implementation of a hierarchical Bayesian model for parameter estimation of a non-trivial psychophysical experiment (inferring the partitioning of internal vs. external noise in depth perception). Posterior computation with Gibbs sampler nested with Metropolis-Hastings algorithm.
  </div>
</div>

---
<h2 class="h1" style="color: rgb(0,0,0)" id="publications">Publications </h2>

* Dale Zhou, Yoona Kang, Danielle Cosme, Mia Jovanova, Xiaosong He, Arun Mahadevan, **Jeesung Ahn**, Ovidia Stanoi, Julia Brynildsen, Nicole Cooper, Eli Cornblath, Linden Parkes, Peter Mucha, Kevin Ochsner, David Lydon-Staley, Emily Falk, Dani Bassett. Mindful attention promotes control of brain network dynamics for self-regulation and discontinues the past from the present. *Proceedings of the National Academy of Sciences (in press)*, 2022.

* Danielle Cosme, Yoona Kang, Jose Carreras Tartak, **Jeesung Ahn**, Faustine E Corbani, Nicole Cooper, Bruce Doré, Xiaosong He, Chelsea Helion, Mia Jovanova, Silicia Lomax, Arun S Mahadevan, Amanda L McGowan, Alexandra Paul, Rui Pei, Anthony Resnick, Ovidia Stanoi, Yi Zhang, Danielle S Bassett, Zachary M Boyd, David Martin Lydon-Staley, Peter Mucha, Kevin N Ochsner, Emily Falk.
[Study protocol: Social Health Impact of Network Effects (SHINE) Study](https://psyarxiv.com/cj2nx/). *PsyArXiv*, 2022.

**(† deonotes co-first authorship)**

---
<h2 class="h1" style="color: rgb(0,0,0)" id="contact-me">Contact </h2>

Richards Medical Research Laboratories,
3700 Hamilton Walk, 5F, Philadelphia, PA 19104 

<p class="home-element"><strong> jeesung [at] sas [dot] upenn [dot] edu</strong></p>

<style type="text/css">
  .body-social > ul {
    display: inline-block;
    list-style-type: none;
    margin-bottom: 0;
    overflow: hidden;
    padding: 0;
  }

  .body-social > ul > li {
    float: left;

    /* padding-left: 5px; */
    padding-right: 10px;

    /* display: inline-block; */
  }

  .body-social > ul > li > a {
    display: inline;
    text-align: center;
    font-size: 0.95rem;
    font-weight: 600;
    /*width: 3rem;*/
    /*height: 4rem;*/
    padding: 4px;

    /* line-height: 3rem; */

    text-decoration: none;
    border-width: 1px;
    border-style: solid;
    border-radius: 5px;
    transition: background-color 250ms, color 250ms, text-decoration-color 250ms, border-color 250ms;

    /* border-bottom: none; */
  }

  .body-social > ul > li > a:not(.btn):not(.no-hover) {
    border-color: var(--accent-color);
  }

  .body-social > ul > li > a:hover {
    color: white;
    background-color: var(--accent-color);
    border-radius: 5px;
    padding: 4px;
    transition: background-color 250ms, color 250ms, text-decoration-color 250ms, border-color 250ms;
  }

  .row {
    display: flex;
  }

  .column {
    flex: 50%;
  }

  img.proj-image {
    display: block;
    margin-right: auto;
    padding-right: 20px;
  }
</style>
